import osimport csvimport matplotlib.pyplot as pltimport randomimport numpy as npimport pandas as pd# import pydotplus as pdpimport seaborn as snsimport torchfrom matplotlib.backends.backend_pdf import PdfPagesfrom sklearn.metrics import aucfrom sklearn.metrics import RocCurveDisplayfrom sklearn.metrics import precision_recall_curvefrom sklearn.metrics import PrecisionRecallDisplayfrom sklearn.model_selection import StratifiedKFold# from dtreeviz.trees import dtreevizfrom sklearn.model_selection import cross_val_predictfrom sklearn import tree, __all__from sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import accuracy_scorefrom sklearn.metrics import balanced_accuracy_scorefrom sklearn.metrics import confusion_matrixfrom sklearn.metrics import f1_score# from sklearn.metrics import plot_confusion_matrixfrom sklearn.metrics import precision_scorefrom sklearn.metrics import recall_scorefrom sklearn.model_selection import ParameterGridfrom sklearn.model_selection import train_test_splitfrom collections import Counterfrom sklearn.datasets import make_classificationfrom imblearn.over_sampling import SMOTEimport shapimport IPythonfrom imblearn.over_sampling import RandomOverSampler, SMOTENC# import categorical_indexfrom sklearn import model_selectionimport tqdmfrom submodules import *def isnumber(x):    try:        float(x)        return True    except:        return Falsedef sum_noHL(arr):    return sum(sorted(arr)[5:-5])# RF parametersmy_max_depth = 7num_tree = 300sample_flag = Falsethe_infile = '../../RFcompare/dataset/descriptor_feature_RF_sorted_normed.csv'the_outfile_tmp = "../../RFcompare/output"# seed = 90000# seeds = [i*1000+i for i in range(21, 26)]seeds = [3003, 300321, 399211]for seed in seeds:    test_hit = 10    test_nonhit = 240    train_hit = 140    train_nonhit = 140    val_hit = 10    val_nonhit =240    sample_test_path = "../../data/origin/test_index_hit_" + str(test_hit) + "_nhit_" + str(test_nonhit) \                       + "_seed_" + str(seed) + ".csv"    sample_hit_path = "../../data/origin/index_hit_seed_" + str(seed) + ".csv"    hit_index_pth = "../../data/origin/hit_index.csv"    with open(sample_hit_path, "r") as f:        reader = csv.reader(f)        hit_index = [row for row in reader]    test_index_pth = "../../data/origin/test_index.csv"    with open(sample_test_path, "r") as f:        reader = csv.reader(f)        test_index = [row for row in reader]    if sample_flag:        train_each_num = 140        train_index_pth = "../../data/origin/train_index_hit_" + str(train_hit) + "_nhit_" + \                                str(train_nonhit) + "_seed_" + str(seed) + ".csv"    else:        train_index_pth = "../../data/origin/train_index_" + str(seed) + ".csv"    with open(train_index_pth, "r") as f:        reader = csv.reader(f)        train_index = [row for row in reader]    val_index_pth = "../../data/origin/val_index_hit_" + str(val_hit) + "_nhit_" + \                                str(val_nonhit) + "_seed_" + str(seed) + ".csv"    with open(val_index_pth, "r") as f:        reader = csv.reader(f)        val_index = [row for row in reader]    # test_hit = 20    # test_nonhit = 480    # train_hit = 140    # train_nonhit = 140    top_k = 20  # save top k features    cv_splits = 3  # cross validation    random_seed = 50000  # seed for reproducibility    # dropped_nonhit = 1450  # total non-hit in training = 1523    dropped_nonhit = 0  # total non-hit in training = 1523    repeat_times = 1  # drop non-hits from training dataset with different initial seed    categorical = categorical_index.categorical_index_fc()  # categorical index for oversampling    rsp = SMOTENC(random_state=6000, categorical_features=categorical)    tree_index = 3  # Should be within [0, n_estimators-1]    # main_info = "org_0.2test_ratio_" + "dep" + str(my_max_depth) + \    #             "tree" + str(num_tree) + "seed" + str(random_seed) + \    #             "drop_nh" + str(dropped_nonhit)    # the_outfile = the_outfile + "/" + main_info    #    # out_feature_rank = the_outfile + "/feature_rank_" + main_info + ".txt"    # saved_trees = "trees_" + str(tree_index) + main_info  # the saved tree visualization    # os.makedirs(the_outfile + "/" + main_info, exist_ok=True)    if __name__ == '__main__':        frame = pd.read_csv(the_infile)  # load data        print(frame)        frame.head()        data = frame.iloc[:, 2:]  # the 1st column is the label identifier        header = list(data.columns)  # input of SHAP explainer        data = data[data.applymap(isnumber)].to_numpy().astype(float)  # set non-numeric values to 0        data[np.isnan(data)] = 0        print("data", data.shape)        data_df = pd.DataFrame(data, columns=header)  # input of SHAP explainer        two_class_separation = True  # If False, go for three-class separation        # generating labels        labels = frame.iloc[:, [1]].to_numpy().flatten()  # grab the first column: "binder xxx" or "non binder xxx"        numeric_labels = np.zeros(labels.shape)  # give all data a "0" label        # nonhit_index = 104        index = np.zeros(len(numeric_labels), dtype=bool)        for id in hit_index:            for i in id:                # print(i)                index[int(i)] = True        if two_class_separation:            numeric_labels[index] = 1  # 1 for hit, 0 for non-hit        else:            numeric_labels[34:67] = 1  # hit_slow labeled as 0, hit_fast as 1            numeric_labels[nonhit_index:] = 2  # 2 for non-hit        index = np.ones(len(numeric_labels), dtype=bool)        index_2 = np.ones(len(numeric_labels), dtype=bool)        index_3 = np.ones(len(numeric_labels), dtype=bool)        for id in test_index:            for i in id:                index[int(i)] = False        for id in train_index:            for i in id:                index_2[int(i)] = False        for id in val_index:            for i in id:                index_3[int(i)] = False        count = 0        # train test data splitting        np.random.seed(random_seed)  # fixing the randomness        test_index = test_index[0]        test_index = [int(a) for a in test_index]        train_index = [int(a) for a in train_index[0]]        val_index = [int(a) for a in val_index[0]]        X_train = data_df.iloc[train_index, :]        X_val = data_df.iloc[val_index, :]        X_test = data_df.iloc[test_index, :]        y_train = numeric_labels[[not x for x in index_2]]        y_val = numeric_labels[[not x for x in index_3]]        y_test = numeric_labels[[not x for x in index]]        n = 0        index_nonhit_trn = []        for i in y_train:            n += 1            if i == 1:                index_nonhit_trn.append(n - 1)        index_nonhit_trn = index_nonhit_trn        print("nonhit in trn", len(index_nonhit_trn))        X_train_reset_id = X_train.reset_index()        y_train_ = y_train        max_recall = 0        min_mis_nonhit = 500        rcd_result = []        rcd_f1 = []        rcd_recall = []        random.seed(random_seed)        coun = 1        my_max_depth = 7        num_tree = 300        # param_grid = {'my_max_depth': [6],        #               'num_tree': [300]}        param_grid = {'my_max_depth': [6, 8, 10, 12, 14, 16],                      'num_tree': [5, 10, 50, 100, 500, 1000]}        grid = ParameterGrid(param_grid)        for params in grid:            main_info = "org_0.2test_ratio_" + "dep" + str(params["my_max_depth"]) + \                        "tree" + str(params["num_tree"]) + "seed" + str(seed) + \                        "drop_nh" + str(dropped_nonhit) + "_0830"            the_outfile = the_outfile_tmp + "/" + main_info            out_feature_rank = the_outfile + "/feature_rank_" + main_info + ".txt"            saved_trees = "trees_" + str(tree_index) + main_info  # the saved tree visualization            os.makedirs(the_outfile, exist_ok=True)            for random_i in random.sample(range(0, 100000), repeat_times):                print("start " + str(coun) + " time.")                coun += 1                clf = RandomForestClassifier(max_depth=params["my_max_depth"], random_state=0, n_estimators=params["num_tree"])                clf.fit(X_train, y_train)                # Visualization                feature_names = pd.read_csv(the_infile, nrows=1).columns                feature_names = list(feature_names[1:])                for i, name in enumerate(feature_names):                    name = name + '[%d]' % i                    feature_names[i] = name                if two_class_separation:                    class_names = ['hit', 'non-hit']                else:                    class_names = ['hit_s', 'hit_f', 'non-hit']                # # this visualization part still under debugging                # dot_data = tree.export_graphviz(clf.estimators_[tree_index],                #                                 out_file=None,                #                                 rounded=True,                #                                 filled=True,                #                                 feature_names=feature_names,                #                                 class_names=class_names,                #                                 rotate=True)                # graph = pdp.graph_from_dot_data(dot_data)                # graph.write_png(the_outfile + "/" + "mac" + saved_trees + '.png')                # Visualization 2                # viz = dtreeviz(clf.estimators_[tree_index],                #                X_train,                #                y_train,                #                feature_names=feature_names,                #                class_names=class_names)                # viz.save(the_outfile + "/" + saved_trees + '.svg')                # # plotting feature importance                importance = clf.feature_importances_  # compute the feature importance                indices = np.argsort(importance)[::-1]  # Sort them in descending order                # Rearrange feature names let they match the sorted feature importance                names = [feature_names[i] for i in indices]                names = np.array(names)                new_indices = indices[:top_k]                with open(out_feature_rank, 'w') as fd:  # write the feature ranking to txt file;                    # "w": an existing file with the same name will be erased                    fd.write("entry" + "," + "feature_name" + "," + "feature_index_check" +                             "," + "feature_importance_value" + "1000*feature_importance_value" + "\n")                    for f in range(top_k):  # loop over the top k features                        fd.write(str(f + 1) + "," + names[f] + "," + str(new_indices[f]) +                                 "," + str(importance[new_indices[f]]) + "," + str(1000 * importance[new_indices[f]]) + "\n")                # plot the result in histogram                plt.figure(figsize=(8, 12), constrained_layout=True, dpi=300)                flipped_importance = np.flip(importance[new_indices], axis=None)  # from horizontal to vertical                plt.barh(range(top_k), flipped_importance,                         color="cornflowerblue", align="center")                flipped_name = np.flip(names[:top_k], axis=None)                plt.yticks(range(top_k), flipped_name, fontsize=15)                plt.xticks(fontsize=13)                plt.gca().spines['top'].set_visible(False)                plt.gca().spines['right'].set_visible(False)                plt.savefig(the_outfile + '/' + 'feature_rank_' + main_info + '.png')                plt.close()                #                # (1) confusion matrix for test dataset                """somehow affect the shap summary plot1,                comment out when plot the SHAP results"""                title_font = 2.6                subplot_size = (6.7, 5.1)                titles_options = [(" ", None),                                  ("normalized", 'true')]  # a list stores two cm titles                label_dict = ('non-hit', 'hit')                pdf_path = the_outfile + "/" + main_info                with PdfPages(pdf_path + ".pdf") as pdf:                    y_pred = clf.predict(X_test)                    y_true = y_test                    plot_confusion_matrix(torch.tensor(y_true, device='cpu'), torch.tensor(y_pred, device='cpu'),                                          classes=label_dict, results_saved_folder=the_outfile, epoch=1,                                          trn_or_test="Test", cmap="Blues")                    pdf.savefig()                    y_pred = clf.predict(X_val)                    y_true = y_val                    plot_confusion_matrix(torch.tensor(y_true, device='cpu'), torch.tensor(y_pred, device='cpu'),                                          classes=label_dict, results_saved_folder=the_outfile, epoch=1,                                          trn_or_test="Validation", cmap="Oranges")                    pdf.savefig()                    y_pred = clf.predict(X_train)                    y_true = y_train                    plot_confusion_matrix(torch.tensor(y_true, device='cpu'), torch.tensor(y_pred, device='cpu'),                                          classes=label_dict, results_saved_folder=the_outfile, epoch=1,                                          title="cfm on training", cmap="Greens")                    pdf.savefig()                # test and evaluation                y_pred = clf.predict(X_test)                cm = confusion_matrix(y_test, y_pred, labels=[0, 1])                with open(the_outfile_tmp + "/test_results_seed_" + str(seed) + ".txt", "w") as f:                    writer = csv.writer(f)                    writer.writerow(test_index)                    writer.writerow(y_test)                    writer.writerow(clf.predict(X_test))                with open(the_outfile_tmp + "/train_results_seed_" + str(seed) + ".txt", "w") as f:                    writer = csv.writer(f)                    writer.writerow(train_index)                    writer.writerow(y_train)                    writer.writerow(clf.predict(X_train))                f1 = f1_score(y_test, y_pred, average=None)                rcd_f1.append(f1[1])  # f1 for hits                recall = recall_score(y_test, y_pred, average=None)                rcd_recall.append(recall[1])  # recall for hits                precision = precision_score(y_test, y_pred, average=None)                if recall[1] > max_recall:                    max_recall = recall[1]                    min_mis_nonhit = 500                    min_mis_time = []                    max_recall_time = []                    max_recall_time.append(str(params["my_max_depth"]) + "_" + str(params["num_tree"]))                    if precision[1] < min_mis_nonhit:                        min_mis_time = []                        min_mis_nonhit = precision[1]                        min_mis_time.append(str(params["my_max_depth"]) + "_" + str(params["num_tree"]))                    elif precision[1] == min_mis_nonhit:                        min_mis_time.append(str(params["my_max_depth"]) + "_" + str(params["num_tree"]))                elif recall[1] == max_recall:                    max_recall_time.append(str(params["my_max_depth"]) + "_" + str(params["num_tree"]))                    if precision[1] < min_mis_nonhit:                        min_mis_time = []                        min_mis_nonhit = precision[1]                        min_mis_time.append(str(params["my_max_depth"]) + "_" + str(params["num_tree"]))                    elif precision[1] == min_mis_nonhit:                        min_mis_time.append(str(params["my_max_depth"]) + "_" + str(params["num_tree"]))                print("confusion matrix_test\n", cm)                print('F1 score is', f1_score(y_test, y_pred, average=None))                print('precision_score is', precision_score(y_test, y_pred, average=None))                print('recall_score is', recall_score(y_test, y_pred, average=None))                acc = balanced_accuracy_score(y_test, y_pred)                print('the prediction accuracy is {:0.1%}'.format(acc))                rcd_result.append(str(random_i) + str(f1) + " rc" + str(recall) + "\n")            h_f1 = max(rcd_f1)            h_recall = max(rcd_recall)            if len(rcd_f1)-10 != 0:                avg_f1 = sum_noHL(rcd_f1)/(len(rcd_f1)-10)  # the avg without highest and lowest value            else:                avg_f1 = 0            if len(rcd_recall)-10 != 0:                avg_recall = sum_noHL(rcd_recall)/(len(rcd_recall)-10)            else:                avg_recall = 0            l_f1 = sorted(rcd_f1)[0]            l_recall = sorted(rcd_recall)[0]            os.makedirs("grid_search_seed", exist_ok=True)            with open("grid_search_seed/down_over_removed-nh" + str(dropped_nonhit)                      + "_seed" + str(random_seed) + "_" + str(num_tree) +"trees_" +                      str(my_max_depth) +"depth"+ ".txt", 'w') as fd:                fd.write("Hits_104," + " Nonhits_" + str(1896-dropped_nonhit)                         + "\nseed: " + str(random_seed)                         + "\n\nAVG recall: " + str(avg_recall)                         + "\nHighest recall: " + str(h_recall)                         + "\nlowest recall: " + str(l_recall)                         + "\n\nAVG f1: " + str(avg_f1)                         + "\nHighest f1: " + str(h_f1)                         + "\nlowest f1: " + str(l_f1)                         + "\n\n-------------------------------"                         + "[seed(drop_nonhit)[(hit), (nonhit)]]\n" + str(rcd_result))            # Classification and ROC curve            X = np.concatenate((X_train, X_test), axis=0)            y = np.concatenate((y_train, y_test), axis=0)            # Run classifier with cross-validation and plot ROC curves            cv = StratifiedKFold(n_splits=cv_splits)            clf_cv = RandomForestClassifier(max_depth=my_max_depth, random_state=0, n_estimators=num_tree)            tprs = []            aucs = []            mean_fpr = np.linspace(0, 1, 100)            fig, ax = plt.subplots(figsize=(12, 11))            ax.set(facecolor="white")            ax.spines['right'].set_color('black')            ax.spines['bottom'].set_color('black')            ax.spines['top'].set_color('black')            ax.spines['left'].set_color('black')            ax.tick_params(labelsize=20)            ax.set_ylabel("True Positive rate", fontsize=20)            ax.set_xlabel("False Positive rate", fontsize=20)            for i, (train, test) in enumerate(cv.split(X, y)):                clf_cv.fit(X[train], y[train])                viz = RocCurveDisplay.from_estimator(                    clf_cv,                    X[test],                    y[test],                    name="ROC fold {}".format(i+1),                    alpha=0.3,                    lw=1,                    ax=ax,                    pos_label=0                )                interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)                interp_tpr[0] = 0.0                tprs.append(interp_tpr)                aucs.append(viz.roc_auc)            ax.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)            mean_tpr = np.mean(tprs, axis=0)            mean_tpr[-1] = 1.0            mean_auc = auc(mean_fpr, mean_tpr)            std_auc = np.std(aucs)            ax.plot(                mean_fpr,                mean_tpr,                color="b",                label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, std_auc),                lw=2,                alpha=0.8,            )            std_tpr = np.std(tprs, axis=0)            tprs_upper = np.minimum(mean_tpr + std_tpr, 1)            tprs_lower = np.maximum(mean_tpr - std_tpr, 0)            ax.fill_between(                mean_fpr,                tprs_lower,                tprs_upper,                color="grey",                alpha=0.2,                label=r"$\pm$ 1 std. dev.",            )            ax.set(                xlim=[-0.05, 1.05],                ylim=[-0.05, 1.05],                # title="Receiver operating characteristic",            )            ax.set_title("Receiver operating characteristic", fontsize=22)            ax.legend(loc="lower right", fontsize=19)            plt.savefig(the_outfile + "/test2_AUC_cv_" + main_info + "_cv" + str(cv_splits) + '.png')            plt.close(fig)        with open(the_outfile_tmp + "/max_recall_time_" + str(seed) + ".txt", "w") as f:            writer = csv.writer(f)            writer.writerow(max_recall_time)        with open(the_outfile_tmp + "/min_mis_time_" + str(seed) + ".txt", "w") as f:            writer = csv.writer(f)            writer.writerow(min_mis_time)